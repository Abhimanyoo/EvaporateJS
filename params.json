{"name":"EvaporateJS","tagline":"Javascript library for browser to S3 multipart resumable uploads","body":"EvaporateJS\r\n===========\r\n\r\nEvaporateJS is a javascript library for directly uploading files from a web browser to AWS S3, using S3's multipart upload. \r\n\r\n###Why?\r\nEvaporateJS can resume an upload after a problem without having to start again at the beginning. For example, let's say you're uploading a 1000MB file, you've uploaded the first 900MBs, and then there is a problem on the network. Normally at this point you'd have to restart the upload from the beginning. Not so with EvaporateJS - it will only redo a small ~5MB chunk of the file, and then carry on from where it left off, and upload the final 100MB.     \r\n\r\nThis is an beta release. It still needs lots more work and testing, but we do use it in production on videopixie.com, and it does reliably upload 20GB+ files.\r\n\r\n\r\n##Set up EvaporateJS\r\n\r\n\r\n1. include evaporate.js in your page (see `example.html`)\r\n\r\n2. setup your S3 bucket (see `s3_cors_example.xml`)\r\n\r\n3. setup a signing handler on your application server (see `signer_example.py`)\r\n\r\n\r\n\r\n##Use EvaporateJS\r\n\r\n\r\nSo far the api contains just two methods, and one property\r\n\r\n###new Evaporate()\r\n\r\n`var evap = new Evaporate(config)`\r\n\r\n\r\n`config` has 3 required properties\r\n\r\n* **signerUrl**:  a url on your application server which will sign a string with your aws secret key. for example 'http://myserver.com/auth_upload'\r\n\r\n* **aws_key**:  your aws key, for example 'AKIAIQC7JOOdsfsdf'\r\n\r\n* **bucket**:  the name of your bucket to which you want the files uploaded , for example 'my.bucket.name'\r\n\r\n\r\n`config` has some optional parameters\r\n\r\n* **logging**: default=true, whether EvaporateJS outputs to the console.log  - should be `true` or `false`\r\n* **maxConcurrentParts**: default=5, how many concurrent file PUTs will be attempted\r\n* **partSize**: default = 6 * 1024 * 1024 bytes, the size of the parts into which the file is broken\r\n* **retryBackoffPower**: default=2, how aggresively to back-off on the delay between retries of a part PUT\r\n* **maxRetryBackoffSecs**: default=20, the maximum number of seconds to wait between retries \r\n* **progressIntervalMS**: default=1000, the frequency (in milliseconds) at which progress events are dispatched\r\n\r\n\r\n###.add()\r\n\r\n`evap.add(config)`\r\n\r\n`config` has 2 required parameters:\r\n\r\n* **name**: _String_. the S3 ObjectName that the completed file will have\r\n* **file**: _File_. a reference to the file object\r\n\r\n`config` has 4 optional parameter:\r\n\r\n\r\n* **xAmzHeadersAtInitiate**: _Object_. an object of key/value pairs that represents the x-amz-... headers that should be added to the initiate POST to S3 (not added to the part PUTS, or the complete POST). An example would be `{'x-amz-acl':'public-read'}`\r\n\r\n* **signParams**: _Object_. an object of key/value pairs that will be passed to _all_ calls to the signerUrl. \r\n\r\n* **complete**: _function()_. a function that will be called when the file upload is complete\r\n\r\n* **progress**: _function(p)_. a function that will be called at a frequency of _progressIntervalMS_ as the file uploads, where _p_ is the fraction (between 0 and 1) of the file that is uploaded. Note that this number will normally increase monotonically, but in the case that one or more parts fails and need to be rePUT, it may go also decrease.\r\n\r\n\r\n###.cancel()\r\n`evap.cancel(id)`\r\n\r\n`id` is the id of the upload that you want to cancel\r\n\r\n###.supported\r\n\r\nThe `supported` property is _Boolean_, and indicates whether the browser has the capabilities required for Evaporate to work. Needs more testing.      \r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}